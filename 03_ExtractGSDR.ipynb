{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f25855",
   "metadata": {},
   "source": [
    "# 03 - Extract GSDR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c73bec",
   "metadata": {},
   "source": [
    "This notebook: \n",
    "- Extracts landslide triggering rainfall for each landslide from each nearby gauge from the GSDR\n",
    "- Extracts annual block maxima at a range of durations from each nearby gauge\n",
    "\n",
    "The outputs of this notebook are:\n",
    "- lsdata_gsdr_rain.csv, a csv with event rainfall (3 hour dry and 48 hour dry periods) from each nearby gauge\n",
    "- annual_block_maxima.csv, a csv with the annual block maxima at a range of durations from each nearby gauge \n",
    "\n",
    "These outputs are read into 05_CombinePrep\n",
    "\n",
    "\n",
    "**Data required**\n",
    "\n",
    "Processed data: ls_get_gsdr_simple.csv\n",
    "\n",
    "Original data: Global Sub-Daily Rainfall Dataset (GSDR).  The publicly available portion of the GSDR is available from the authors upon request.\n",
    "\n",
    "Lewis, E. et al. GSDR: A Global Sub-Daily Rainfall Dataset. Journal of Climate 32, 4715â€“4729 (2019).\n",
    "\n",
    "Lewis, E. et al. Quality control of a global hourly rainfall dataset. Environmental Modelling & Software 144, 105169 (2021).\n",
    "\n",
    "*This notebook requires the intense python package, which handles the GSDR data.  This is available with the public GSDR data upon request from the authors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7829208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import intense\n",
    "from timezonefinder import TimezoneFinder\n",
    "import datetime\n",
    "import warnings\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec33d79",
   "metadata": {},
   "source": [
    "**set directories for data and results directories here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6849556",
   "metadata": {},
   "source": [
    "*note, the two output files from are named 'lsdata_gsdr_rain.csv' and 'annual_block_maxima.csv', they will be saved in the resultsdir.*\n",
    "*this script will also create many of text files with the extracted intense data from the zip files.  These can safely be deleted after the run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set directory where landslide data is here\n",
    "datadir = ''\n",
    "\n",
    "#set directory where outputs should be saved here\n",
    "resultsdir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set directory where GSDR data lives here\n",
    "\n",
    "#gsdrdir = datadir + '20220615_GSDR/GSDR-20220615T144339Z-001/GSDR/Raw data in intense format/'\n",
    "gsdrdir = datadir + '20220615_GSDR/GSDR-20220615T144339Z-001/GSDR/QC_d data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef705012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set mode, whether to run on the publicly available GSDR data or on all GSDR data\n",
    "#mode = 'public' \n",
    "mode = 'all' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read geopandas dataframe with landslides and the gauges that should be accessed for each one\n",
    "\n",
    "lsdata = pd.read_csv(datadir + 'ls_get_gsdr_simple.csv')\n",
    "\n",
    "lsdata['date_local_midnight_utc'] = pd.to_datetime(lsdata['date_local_midnight_utc'])\n",
    "lsdata['StartDate'] = pd.to_datetime(lsdata['StartDate'], utc = True)\n",
    "lsdata['EndDate'] = pd.to_datetime(lsdata['EndDate'], utc = True)\n",
    "\n",
    "lsdata.drop(['Unnamed: 0'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0231b7",
   "metadata": {},
   "source": [
    "### For each landslide, read the gauge data, extract rainfall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ts(lspt, gsdrdir, daysbefore = 90, daysafter = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to read the precipitation time series from the gauge associated with the landslide point,\n",
    "    convert it to UTC, interpolate missing values, and subset to the specified time interval before\n",
    "    and after the landslide point\n",
    "    \n",
    "    lspt = row of a dataframe that contains columns: \n",
    "        - 'Folder' - folder containing the data (e.g. US.zip)\n",
    "        - 'date_local_midnight_utc' - midnight local time on the day the landslide occurred, converted to UTC\n",
    "    \n",
    "    gsdrdir = directory containing the GSDR subfolders\n",
    "    daysbefore = how many days before the landslide should data be extracted?\n",
    "    daysafter = how many days after midnight on the day of the landslide should be extracted? \n",
    "    \n",
    "    Returns: \n",
    "    ts_sub = original time series, subset\n",
    "    ts_fill = original time series, subset and interpolated   \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #get the filename for the station we need\n",
    "    \n",
    "    zipfn = gsdrdir + lspt['Folder']\n",
    "    \n",
    "    z = zipfile.ZipFile(zipfn, \"r\")\n",
    "\n",
    "    tsfn = z.extract(lspt['OriginalID'] + '.txt')\n",
    "    \n",
    "    s = intense.readIntense(tsfn)\n",
    "\n",
    "    #find the right time zone for the gauge \n",
    "\n",
    "    #initiate timezone finder\n",
    "\n",
    "    tf = TimezoneFinder()\n",
    "\n",
    "    if s.time_zone == 'UTC': \n",
    "        stz = 'UTC'\n",
    "        #time zone aware in UTC\n",
    "        ts = s.data.tz_localize(stz, \n",
    "                            ambiguous=np.zeros(len(s.data), dtype = bool), #if it's ambiguous, assign to standard time\n",
    "                           nonexistent ='shift_forward') #if it doesn't exist, shift to next time forward\n",
    "\n",
    "\n",
    "    else:\n",
    "        stz = tf.timezone_at(lng=s.longitude, lat=s.latitude)\n",
    "         #first, localize to local time zone\n",
    "        ts = s.data.tz_localize(stz, \n",
    "                                ambiguous=np.zeros(len(s.data), dtype = bool), \n",
    "                                nonexistent ='shift_forward')\n",
    "        #then, convert to UTC\n",
    "\n",
    "        ts = ts.tz_convert('UTC')\n",
    "\n",
    "\n",
    "    #subset to specified number of days before and after the landslide\n",
    "    lsdate = lspt['date_local_midnight_utc']\n",
    "    \n",
    "    #check that these dates are in the range of the time series\n",
    "    \n",
    "    start_ts = lsdate - datetime.timedelta(days = daysbefore)\n",
    "    end_ts = lsdate + datetime.timedelta(days = daysafter + 1)\n",
    "    \n",
    "    if (start_ts > lspt['StartDate']) & (end_ts < lspt['EndDate']):\n",
    "\n",
    "        ts_sub =  ts.loc[(lsdate - datetime.timedelta(days = daysbefore)):(lsdate + datetime.timedelta(days = daysafter + 1))]\n",
    "\n",
    "        #sort index, just in case for some reason it wasn't already in sorted, or pandas thinks it isn't\n",
    "        ts_sub.sort_index(axis = 0, inplace = True)\n",
    "\n",
    "        #interpolate to fill nans in the time series using the pandas built in interpolation method\n",
    "\n",
    "        ts_fill = ts_sub.interpolate(method = 'time')\n",
    "\n",
    "        #remove any duplicates by keeping the first duplicated entry only\n",
    "        #CAUTION: this could create some problems where we miss an hour of rainfall, but I don't know \n",
    "        #how else to deal with it at this point\n",
    "        ts_fill = ts_fill[~ts_fill.index.duplicated()]\n",
    "    \n",
    "    else: #case that the time series can't be extracted because time series is not completely covered\n",
    "        ts_sub = np.array([])\n",
    "        ts_fill = np.array([])\n",
    "    \n",
    "    return ts_sub, ts_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd663b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triggering_rain(nhdry, ts, ts_raw, lsdate): \n",
    "    \n",
    "    \"\"\"\n",
    "    Gets length and sum of triggering rainfall event from a dry period of length nhdry to the peak hourly\n",
    "    rainfall and the end of the landslide day\n",
    "    \n",
    "    nhdry = length of dry period preceeding landslide in hours\n",
    "    ts = time series with hourly datetime index and values of hourly precip\n",
    "    ts_raw = original time series to check how many nans were interpolated in the time range\n",
    "    lsdate = datetime corresponding to the start of the landslide day  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #if the time series is completely empty\n",
    "    \n",
    "    if len(ts) == 0:\n",
    "        \n",
    "        idxtrst = np.nan\n",
    "        htrsttopk = np.nan\n",
    "        cptrsttopk = np.nan\n",
    "        htrsttoeod = np.nan\n",
    "        cptrsttoeod = np.nan\n",
    "        mwtopk = np.nan\n",
    "        mxhr = np.nan\n",
    "        idxmxhr = np.nan\n",
    "        nnantopk = np.nan\n",
    "        nnantoeod = np.nan\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        #find peak hourly rain on the day of the landslide \n",
    "\n",
    "        mxhr = ts.loc[lsdate:lsdate+datetime.timedelta(hours = 24)].max()\n",
    "       \n",
    "\n",
    "        #if peak hourly rain is nan (e.g. there's no data on the day of the landslide, then)\n",
    "\n",
    "        if pd.isnull(mxhr):\n",
    "            \n",
    "            \n",
    "            idxmxhr = np.nan\n",
    "            mxhr = np.nan\n",
    "            idxtrst = np.nan\n",
    "            htrsttopk = np.nan\n",
    "            cptrsttopk = np.nan\n",
    "            htrsttoeod = np.nan\n",
    "            cptrsttoeod = np.nan\n",
    "            mwtopk = np.nan\n",
    "            nnantopk = np.nan\n",
    "            nnantoeod = np.nan\n",
    "            \n",
    "\n",
    "        else:\n",
    "            \n",
    "            #find out when the peak occurred\n",
    "            idxmxhr = ts.loc[lsdate:lsdate+datetime.timedelta(hours = 24)].idxmax()\n",
    "\n",
    "\n",
    "            #moving window summing, index is last observation in window (eg. 10 am is 8am + 9am + 10 am)\n",
    "            mw = ts.rolling(nhdry).sum() \n",
    "\n",
    "            #subset moving window time series up to the peak \n",
    "            mwtopk = mw.loc[:idxmxhr]\n",
    "\n",
    "            # find where the nhdry hourly sum is less than 0.01 mm (essentially 0, but catch very small values)\n",
    "            #, find the nearest index to the peak point (iloc type index)\n",
    "            ix = mwtopk[mwtopk<0.01].index.get_indexer([idxmxhr], method = 'pad')[0]\n",
    "\n",
    "            #if we can't find a dry enough period in the time series:\n",
    "            if ix == -1:\n",
    "\n",
    "                idxtrst = np.nan\n",
    "                htrsttopk = np.nan\n",
    "                cptrsttopk = np.nan\n",
    "                htrsttoeod = np.nan\n",
    "                cptrsttoeod = np.nan\n",
    "                mwtopk = np.nan\n",
    "                nnantopk = np.nan\n",
    "                nnantoeod = np.nan\n",
    "                \n",
    "\n",
    "            else:\n",
    "\n",
    "                #get the time stamp index of the start of the triggering rainfall \n",
    "                idxtrst = mwtopk[mwtopk<0.01].index[ix] + datetime.timedelta(hours = 1)\n",
    "\n",
    "                #how long did it rain between the start of the triggering rain and the peak (inclusive)? \n",
    "\n",
    "                htrsttopk = ts.loc[idxtrst:idxmxhr].count()\n",
    "\n",
    "                #how much did it rain between the start of the triggering rain and the peak (inclusive)?\n",
    "                cptrsttopk = ts.loc[idxtrst:idxmxhr].sum()\n",
    "\n",
    "                #how long did it rain between the start of the triggering rain and the end of the landslide day? \n",
    "\n",
    "                htrsttoeod = ts.loc[idxtrst:lsdate+datetime.timedelta(hours = 24)].count()\n",
    "\n",
    "                #how much did it rain between the start of the triggering rain and the end of the landslide day?\n",
    "                cptrsttoeod = ts.loc[idxtrst:lsdate+datetime.timedelta(hours = 24)].sum()\n",
    "                \n",
    "                \n",
    "                #check what percent of the original time series leading up to the peak is nans\n",
    "                \n",
    "                nnantopk = ts_raw.loc[idxtrst:idxmxhr].isna().sum()\n",
    "                \n",
    "                #check what percent of the original time series leading up to the end of day is nans\n",
    "                \n",
    "                nnantoeod = ts_raw.loc[idxtrst:lsdate+datetime.timedelta(hours = 24)].isna().sum()\n",
    "                             \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "    return [idxmxhr, \n",
    "            mxhr, \n",
    "            idxtrst, \n",
    "            htrsttopk, \n",
    "            cptrsttopk, \n",
    "            htrsttoeod, \n",
    "            cptrsttoeod, \n",
    "            mwtopk, \n",
    "            nnantopk,\n",
    "            nnantoeod]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antecedent(ts, idxdt, h):\n",
    "    \n",
    "    \"\"\"\n",
    "    get cumulative antecedent precipitation before some hour (e.g. peak precip on landslide day \n",
    "    or before start of triggering rainfall)\n",
    "    \n",
    "    ts = time series\n",
    "    idxdt = datetime that you want antecedent for\n",
    "    h = number of antecedent hours\n",
    "    \n",
    "    \"\"\"\n",
    "    #for the case that there no start to the triggering rainfall was found\n",
    "    if pd.isnull(idxdt):\n",
    "        ante = np.nan\n",
    "        \n",
    "    #for the case that the time series is empty\n",
    "    elif len(ts) == 0:\n",
    "        ante = np.nan\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ante = ts.loc[idxdt - datetime.timedelta(hours = h):idxdt].sum()\n",
    "    \n",
    "    return ante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rainmetrics(lspt, gsdrdir):\n",
    "    \n",
    "    \"\"\"\n",
    "    get info about triggering, event, and antecedent rainfall for a landslide\n",
    "    \n",
    "    lspt - a dataframe row from lsdata\n",
    "    \n",
    "    \"\"\"\n",
    "    print(lspt.name)\n",
    "    \n",
    "    [ts, ts_fill] = prep_ts(lspt, gsdrdir, daysbefore = 90, daysafter = 1)\n",
    "    \n",
    "    if ts is None: \n",
    "        \n",
    "        rd = None\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        lsdate = lspt['date_local_midnight_utc']\n",
    "\n",
    "        #then, get triggering rainfall metrics\n",
    "\n",
    "        #define triggering event as 3 hours dry to peak or end of day\n",
    "        [idxmxhr, mxhr, idxtrst, \n",
    "         htrsttopk, cptrsttopk, htrsttoeod, cptrsttoeod, _, nnantrsttopk, nnantrsttoeod] = triggering_rain(nhdry = 3,\n",
    "                                                                                                   ts = ts_fill,\n",
    "                                                                                                    ts_raw = ts,\n",
    "                                                                                                   lsdate = lsdate)\n",
    "\n",
    "        #get event metrics \n",
    "\n",
    "        #define event as 48 hours dry to peak or end of day\n",
    "        [_, _, idxest, \n",
    "         hesttopk, cpesttopk, hesttoeod, cpesttoeod, _, nnanesttopk, nnanesttoeod] = triggering_rain(nhdry = 48,\n",
    "                                                                                ts = ts_fill,\n",
    "                                                                                ts_raw = ts,\n",
    "                                                                                lsdate = lsdate)\n",
    "\n",
    "\n",
    "        #get antecedent metrics \n",
    "\n",
    "        #24 hours before start of triggering rain \n",
    "\n",
    "        ante24htrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24)\n",
    "\n",
    "        #7 day before start of triggering rain \n",
    "        ante7dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*7)\n",
    "\n",
    "        #14 day before start of triggering rain \n",
    "        ante14dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*14)\n",
    "\n",
    "        #21 day before start of triggering rain \n",
    "        ante21dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*21)\n",
    "\n",
    "        #28 day before start of triggering rain\n",
    "        ante28dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*28)\n",
    "\n",
    "\n",
    "        #get estimated exceedance probabilities of the hourly maximum intensity\n",
    "\n",
    "       # mxhrexprob = get_exceedanceprob(ts, mxhr, 10) #minimum 10 years on record\n",
    "\n",
    "\n",
    "        #put everything into a dictionary to return \n",
    "\n",
    "\n",
    "        rd = {\"event_id\":lspt.name, \n",
    "             \"tr_start\":idxtrst,\n",
    "             \"tr_tpk\":idxmxhr,\n",
    "             \"tr_ppk\":mxhr,\n",
    "             \"tr_htopk\":htrsttopk, \n",
    "             \"tr_cptopk\":cptrsttopk, \n",
    "             \"tr_htoeod\":htrsttoeod, \n",
    "             \"tr_cptoeod\":cptrsttoeod,\n",
    "             \"tr_nnantopk\": nnantrsttopk, \n",
    "             \"tr_nnan_toeod\": nnantrsttoeod,\n",
    "             \"e_start\":idxest, \n",
    "             \"e_htopk\":hesttopk, \n",
    "             \"e_cptopk\":cpesttopk, \n",
    "             \"e_htoeod\":hesttoeod, \n",
    "             \"e_cptoeod\":cpesttoeod,\n",
    "             \"e_nnantopk\":nnanesttopk,\n",
    "             \"e_nnantoeod\":nnanesttoeod,               \n",
    "             \"tr_ante24h\":ante24htrst, \n",
    "             \"tr_ante7d\":ante7dtrst, \n",
    "             \"tr_ante14d\":ante14dtrst, \n",
    "             \"tr_ante21d\":ante21dtrst, \n",
    "             \"tr_ante28d\":ante28dtrst}\n",
    "\n",
    "    print(lspt.name)\n",
    "\n",
    "    return rd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a733e2",
   "metadata": {},
   "source": [
    "### Get GSDR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the landslide data for which we already have data in the public dataset for now \n",
    "\n",
    "def publicGSDR(lspt):\n",
    "    \n",
    "    folders = ['Belgium.zip', \n",
    "               'Finland.zip', \n",
    "               'Germany.zip', \n",
    "               'Ireland.zip', \n",
    "               'ISD.zip',\n",
    "               'Japan.zip', \n",
    "               'Norway.zip', \n",
    "               'UK.zip', \n",
    "               'US.zip']\n",
    "    \n",
    "    public = lspt['Folder'] in folders \n",
    "    \n",
    "    return public\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c089fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'public':\n",
    "    lsdata['public'] = lsdata.apply(lambda lspt:publicGSDR(lspt), axis = 1)\n",
    "    lsdata_gsdr = lsdata[lsdata['public']].copy()\n",
    "    \n",
    "else: \n",
    "    lsdata_gsdr = lsdata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c354999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample a subset of points for testing\n",
    "#lsdata_gsdr = lsdata_gsdr.sample(20, axis = 0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lsdata_gsdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record = True):\n",
    "    lsdata_gsdr['rain_metrics'] = lsdata_gsdr.apply(lambda lspt:get_rainmetrics(lspt, gsdrdir), \n",
    "                                             axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44922249",
   "metadata": {},
   "outputs": [],
   "source": [
    "joindf = pd.DataFrame(list(lsdata_gsdr['rain_metrics'].values))\n",
    "\n",
    "lsdata_gsdr_rain = pd.concat([lsdata_gsdr, \n",
    "             joindf.set_index(lsdata_gsdr.index)], \n",
    "             axis = 1)\n",
    "\n",
    "lsdata_gsdr_rain.drop(['rain_metrics'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc078fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip city names of white space, special characters, etc for R\n",
    "\n",
    "lsdata_gsdr_rain['city'] = lsdata_gsdr_rain.apply(lambda row:''.join(e for e in row['UC_NM_MN'] if e.isalnum()), \n",
    "                                       axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdata_gsdr_rain.to_csv(resultsdir + 'lsdata_gsdr_rain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cdedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsdata_gsdr_rain = pd.read_csv(resultsdir + 'lsdata_gsdr_rain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdata_gsdr_rain.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f1acd",
   "metadata": {},
   "source": [
    "### Get annual block maxima from the gauges associated with landslides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_whole_ts(station, gsdrdir):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to read the precipitation time series from a gauge, convert it to UTC,\n",
    "    interpolate missing values\n",
    "    \n",
    "    station = row of a dataframe that contains columns: \n",
    "        - 'Folder' - folder containing the data (e.g. US.zip)\n",
    "        - 'OriginalID' - name of station\n",
    "    gsdrdir = directory containing the GSDR subfolders\n",
    "    \n",
    "    \n",
    "    Returns: \n",
    "    ts = original time series\n",
    "    ts_fill = original time series, interpolated   \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #get the filename for the station we need\n",
    "    \n",
    "    zipfn = gsdrdir + station['Folder']\n",
    "    \n",
    "    z = zipfile.ZipFile(zipfn, \"r\")\n",
    "\n",
    "    tsfn = z.extract(station['OriginalID'] + '.txt')\n",
    "    \n",
    "    s = intense.readIntense(tsfn)\n",
    "\n",
    "    #find the right time zone for the gauge \n",
    "\n",
    "    #initiate timezone finder\n",
    "\n",
    "    tf = TimezoneFinder()\n",
    "\n",
    "    if s.time_zone == 'UTC': \n",
    "        stz = 'UTC'\n",
    "        #time zone aware in UTC\n",
    "        ts = s.data.tz_localize(stz, \n",
    "                            ambiguous=np.zeros(len(s.data), dtype = bool), #if it's ambiguous, assign to standard time\n",
    "                           nonexistent ='shift_forward') #if it doesn't exist, shift to next time forward\n",
    "\n",
    "\n",
    "    else:\n",
    "        stz = tf.timezone_at(lng=s.longitude, lat=s.latitude)\n",
    "         #first, localize to local time zone\n",
    "        ts = s.data.tz_localize(stz, \n",
    "                                ambiguous=np.zeros(len(s.data), dtype = bool), \n",
    "                                nonexistent ='shift_forward')\n",
    "        #then, convert to UTC\n",
    "\n",
    "        ts = ts.tz_convert('UTC')\n",
    "\n",
    "\n",
    "    #sort index, just in case for some reason it wasn't already in sorted, or pandas thinks it isn't\n",
    "    ts.sort_index(axis = 0, inplace = True)\n",
    "    \n",
    "    #interpolate to fill nans in the time series using the pandas built in interpolation method\n",
    "\n",
    "    ts_fill = ts.interpolate(method = 'time')\n",
    "\n",
    "    #remove any duplicates by keeping the first duplicated entry only\n",
    "    #CAUTION: this could create some problems where we miss an hour of rainfall, but I don't know \n",
    "    #how else to deal with it at this point\n",
    "    ts_fill = ts_fill[~ts_fill.index.duplicated()]\n",
    "\n",
    "    return ts, ts_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c6688",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquestations = lsdata_gsdr_rain['NewID'].unique() #stations with a landslide associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f297f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [1, 3, 6, 12, 24, 48, 100, 200, 500, 1000] #durations to extract block maxima for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e693f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uniquestations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84965c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record = True):\n",
    "#loop through all stations associated with a landslide and extract annual block maxima at a range of durations\n",
    "\n",
    "    #loop over stations\n",
    "\n",
    "    for s in range(len(uniquestations)):\n",
    "        \n",
    "        print(s)\n",
    "\n",
    "        station = lsdata_gsdr_rain[lsdata_gsdr_rain['NewID'] == uniquestations[s]].iloc[0][['ID_HDC_G0', \n",
    "                                                                                          'UC_NM_MN', \n",
    "                                                                                          'Folder', \n",
    "                                                                                          'OriginalID', \n",
    "                                                                                           'NewID', \n",
    "                                                                                          'Latitude',\n",
    "                                                                                          'Longitude',\n",
    "                                                                                          'Recordlength(hours)',\n",
    "                                                                                          'Recordlength(years)',\n",
    "                                                                                          'StartDate',\n",
    "                                                                                          'EndDate',\n",
    "                                                                                          'Missingdata(%)',\n",
    "                                                                                          'geometry_y']]\n",
    "        #get time series\n",
    "\n",
    "        ts, ts_fill = prep_whole_ts(station, gsdrdir)\n",
    "\n",
    "        #extract block maxima at a range of durations\n",
    "\n",
    "        for d in range(len(durations)):\n",
    "\n",
    "           #moving window on the raw time series\n",
    "            mw = ts.rolling(durations[d]).sum() #right index is sum of previous d observations. \n",
    "            #if there are nans within the window, returns nan\n",
    "\n",
    "            ann_block_max = mw.resample('Y').max() #take annual max. ignores nans (could be missing data, \n",
    "            #will still get max)\n",
    "\n",
    "\n",
    "            #moving window on the interpolated time series\n",
    "\n",
    "            mw_fill = ts_fill.rolling(durations[d]).sum() #right index is sum of previous d observations. \n",
    "            #if there are nans within the window, returns nan\n",
    "\n",
    "            ann_block_max_fill = mw_fill.resample('Y').max() #take annual max. ignores nans (could be missing data, \n",
    "            #will still get max)\n",
    "\n",
    "\n",
    "            block_max_df = pd.DataFrame(ann_block_max, columns = ['raw_block_max'])\n",
    "            block_max_df['fill_block_max'] = ann_block_max_fill.values\n",
    "\n",
    "            #record the number of non-nan observations in the moving window (all other hours in the year are then nan)\n",
    "            block_max_df['raw_notnainmw'] = mw.resample('Y').count().values\n",
    "\n",
    "            #record the number of non-nan observations in the raw time series (all other hours in the year are then nan)\n",
    "            block_max_df['raw_notnaints'] = ts.resample('Y').count().values\n",
    "\n",
    "\n",
    "            block_max_df['duration(h)'] = durations[d]\n",
    "            block_max_df['year'] = block_max_df.index.year\n",
    "\n",
    "\n",
    "            if d == 0:\n",
    "\n",
    "                stationdf = block_max_df.copy() \n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                stationdf = pd.concat([stationdf, block_max_df])\n",
    "\n",
    "\n",
    "        #assign information about the station\n",
    "        stationinfo = pd.DataFrame([station])\n",
    "\n",
    "        stationinfo = stationinfo.loc[stationinfo.index.repeat(len(stationdf))]\n",
    "\n",
    "        stationinfo.set_index(stationdf.index, inplace = True)\n",
    "\n",
    "        stationdf = pd.concat([stationinfo, stationdf], axis = 1)\n",
    "\n",
    "        if s == 0: \n",
    "\n",
    "            maindf = stationdf.copy()\n",
    "\n",
    "        else: \n",
    "\n",
    "            maindf = pd.concat([maindf, stationdf])\n",
    "            \n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eefbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip city names of white space, special characters, etc for R\n",
    "\n",
    "maindf['city'] = maindf.apply(lambda row:''.join(e for e in row['UC_NM_MN'] if e.isalnum()), \n",
    "                                       axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "maindf.to_csv(resultsdir + 'annual_block_maxima.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a31333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maindf = pd.read_csv(resultsdir + 'annual_block_maxima.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
