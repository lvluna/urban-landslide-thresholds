{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad676f3",
   "metadata": {},
   "source": [
    "# 04 - Extract Durban and Medellin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf35e9",
   "metadata": {},
   "source": [
    "This notebook extracts metrics for landslides in Durban and Medellin. It's similar to 03_ExtractGSDR, but processes precipitation data for Durban from the South African Weather Service and for Medellin from the Colombian Instituto de Hidrología, Meteorología y Estudios Ambientales (IDEAM).  \n",
    "\n",
    "The outputs of this notebook are: \n",
    "\n",
    "- lsdata_durban_rain.csv\n",
    "- annual_block_maxima_durban.csv\n",
    "- lsdata_medellin_rain.csv\n",
    "- annual_block_maxima_medellin.csv\n",
    "\n",
    "These outputs are read into 05_CombinePrep\n",
    "\n",
    "**Data required**\n",
    "\n",
    "Processed data: ls_urban_ts_rf_u.pkl\n",
    "\n",
    "Original data: \n",
    "\n",
    "*Durban*\n",
    "\n",
    "These data were obtained from the South African Weather Service and used with permission for this study\n",
    "- durban_hourly_rainfall_data.xls \n",
    "- durban_station_locations.xls \n",
    "\n",
    "*Medellin* \n",
    "\n",
    "Data for Medellín were obtained from the Colombian Instituto de Hidrología, Meteorología y Estudios Ambientales (IDEAM), were used with permission for this study, and are available at http://dhime.ideam.gov.co/atencionciudadano/.\n",
    "- stations and hourly rainfall data from IDEAM.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import shapely\n",
    "from shapely.ops import transform\n",
    "import datetime\n",
    "import warnings\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from timezonefinder import TimezoneFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = ''\n",
    "\n",
    "resultsdir = ''\n",
    "\n",
    "landslides = pd.read_pickle(resultsdir + 'ls_urban_ts_rf_u.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ead37",
   "metadata": {},
   "source": [
    "### Read and process rainfall data from Durban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    sheet = \"Sheet\" + str(i+1)\n",
    "    print(sheet)\n",
    "    \n",
    "    tempdf = pd.read_excel(\"durban_hourly_rainfall_data.xls\", \n",
    "                        sheet_name = sheet, \n",
    "                        header = 0, \n",
    "                        usecols = \"A:C\")\n",
    "\n",
    "    \n",
    "    if i == 0: \n",
    "        durban_rain = tempdf.copy()\n",
    "        \n",
    "    else:\n",
    "        durban_rain = pd.concat([durban_rain, \n",
    "                               tempdf], \n",
    "                               axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce667f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "durban_rain = durban_rain.set_index(pd.to_datetime(durban_rain['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c00a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read names and locations of Durban stations \n",
    "\n",
    "durban_stations = pd.read_excel(\"durban_station_locations.xlsx\", \n",
    "                        sheet_name = \"Sheet1\", \n",
    "                        header = 0, \n",
    "                        usecols = \"A:D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to a geopandas dataframe\n",
    "durban_stations = gpd.GeoDataFrame(durban_stations,\n",
    "                                 geometry=gpd.points_from_xy(durban_stations.longitude, durban_stations.latitude),\n",
    "                                 crs = \"EPSG:4326\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0b610",
   "metadata": {},
   "source": [
    "Convert local timestamp to UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate timezone finder\n",
    "tf = TimezoneFinder()\n",
    "\n",
    "stz = tf.timezone_at(lng=durban_stations.iloc[0].longitude, lat=durban_stations.iloc[0].latitude)\n",
    "\n",
    "#first, localize to local time zone\n",
    "durban_rain = durban_rain.tz_localize(stz, \n",
    "                        ambiguous=np.zeros(len(durban_rain), dtype = bool), \n",
    "                        nonexistent ='shift_forward')\n",
    "#then, convert to UTC\n",
    "\n",
    "durban_rain = durban_rain.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b7355",
   "metadata": {},
   "source": [
    "Get Durban landslides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "durban_landslides = landslides[landslides['UC_NM_MN'] == 'Durban'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9635e",
   "metadata": {},
   "source": [
    "functions for Durban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3014d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function \n",
    "\n",
    "def get_durban_stations(lspt, \n",
    "                durban_stations,\n",
    "                buffer_dist = 25000):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to get the all of the South African Weather Service stations \n",
    "    within a defined buffer distance to a landslide point\n",
    "    \n",
    "    lspt = a landslide point (with a DATE and spatial location in WGS84)\n",
    "    durban_stations = pandas dataframe Station_Name and a geometry in WGS84\n",
    "    buffer_dist = buffer distance in meters\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "\n",
    "    #define azimuthal equidistant crs centered on the landslide\n",
    "\n",
    "    aeqd = pyproj.Proj(proj='aeqd', ellps='WGS84', datum='WGS84', lat_0=lspt.geometry.y, lon_0=lspt.geometry.x).srs\n",
    "\n",
    "    #reproject the landslide point\n",
    "\n",
    "    project = pyproj.Transformer.from_crs(wgs84, aeqd, always_xy=True).transform\n",
    "    lspt_a = transform(project, lspt.geometry)\n",
    "\n",
    "\n",
    "    #buffer the landlide point\n",
    "    lspt_ab = lspt_a.buffer(buffer_dist)\n",
    "\n",
    "    #reproject the station data \n",
    "    ga = durban_stations.to_crs(aeqd)\n",
    "\n",
    "    #get the indices of all stations within 25 km of the landslide point\n",
    "    sidx = ga.sindex.query(lspt_ab, predicate = 'intersects')\n",
    "    \n",
    "    #columns of the metadata dataframe to use as dictionary keys\n",
    "    \n",
    "    gscols = durban_stations.columns\n",
    "    \n",
    "    #check if it's empty\n",
    "    \n",
    "    if len(sidx) == 0: \n",
    "        \n",
    "        gageinfo = dict(zip(gscols, [None]*len(gscols)))\n",
    "        \n",
    "        gageinfo['flag'] = 'no close gages'\n",
    "        \n",
    "        gageinfodf = pd.DataFrame([gageinfo])\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        #continue\n",
    "\n",
    "        #these are the stations within the buffer distance of the landslide point\n",
    "        gs = ga.iloc[sidx].copy()\n",
    "\n",
    "        #get the distance from the stations to the landslide point\n",
    "        dsl = gs.distance(lspt_a)\n",
    "\n",
    "        #record the distance\n",
    "        gs.loc[dsl.index, 'station_dist'] = dsl.values \n",
    "\n",
    "        #project stations back to WGS84\n",
    "\n",
    "        gs = gs.to_crs(wgs84)\n",
    "\n",
    "        #return the info for the nearest station \n",
    "\n",
    "        gageinfo = gs\n",
    "\n",
    "        gageinfo['flag'] = 'coverage'\n",
    "\n",
    "        gageinfodf = gageinfo.copy()\n",
    "            \n",
    "    gageinfodf['lsidx'] = lspt.name\n",
    "                \n",
    "    return gageinfodf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ts_durban(lspt, durban_rain, daysbefore = 90, daysafter = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to interpolate missing values, and subset to the specified time interval before\n",
    "    and after the landslide point\n",
    "    \n",
    "    lspt = row of a dataframe that contains columns: \n",
    "        - 'Folder' - folder containing the data (e.g. US.zip)\n",
    "        - 'date_local_midnight_utc' - midnight local time on the day the landslide occurred, converted to UTC\n",
    "    \n",
    "    durban_rain = pandas dataframe of durban rain\n",
    "    daysbefore = how many days before the landslide should data be extracted?\n",
    "    daysafter = how many days after midnight on the day of the landslide should be extracted? \n",
    "    \n",
    "    Returns: \n",
    "    ts_sub = original time series, subset\n",
    "    ts_fill = original time series, subset and interpolated   \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #get the time series from the correct station\n",
    "    ts = durban_rain[durban_rain['Station_Name'] == lspt['station_name']]['Rain']\n",
    "\n",
    "    #subset to specified number of days before and after the landslide\n",
    "    lsdate = lspt['date_local_midnight_utc']\n",
    "\n",
    "    #check that these dates are in the range of the time series\n",
    "\n",
    "    start_ts = lsdate - datetime.timedelta(days = daysbefore)\n",
    "    end_ts = lsdate + datetime.timedelta(days = daysafter + 1)\n",
    "\n",
    "\n",
    "    ts_sub =  ts.loc[(lsdate - datetime.timedelta(days = daysbefore)):(lsdate + datetime.timedelta(days = daysafter + 1))]\n",
    "\n",
    "    #sort index, just in case for some reason it wasn't already in sorted, or pandas thinks it isn't\n",
    "    ts_sub.sort_index(axis = 0, inplace = True)\n",
    "\n",
    "    #interpolate to fill nans in the time series using the pandas built in interpolation method\n",
    "\n",
    "\n",
    "    ts_fill = ts_sub.interpolate(method = 'time')\n",
    "\n",
    "    #remove any duplicates by keeping the first duplicated entry only\n",
    "    \n",
    "    ts_fill = ts_fill[~ts_fill.index.duplicated()]\n",
    "        \n",
    "\n",
    "    return ts_sub, ts_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triggering_rain(nhdry, ts, ts_raw, lsdate): \n",
    "    \n",
    "    \"\"\"\n",
    "    Gets length and sum of triggering rainfall event from a dry period of length nhdry to the peak hourly\n",
    "    rainfall and the end of the landslide day\n",
    "    \n",
    "    nhdry = length of dry period preceeding landslide in hours\n",
    "    ts = time series with hourly datetime index and values of hourly precip\n",
    "    ts_raw = original time series to check how many nans were interpolated in the time range\n",
    "    lsdate = datetime corresponding to the start of the landslide day  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #if the time series is completely empty\n",
    "    \n",
    "    if len(ts) == 0:\n",
    "        \n",
    "        idxtrst = np.nan\n",
    "        htrsttopk = np.nan\n",
    "        cptrsttopk = np.nan\n",
    "        htrsttoeod = np.nan\n",
    "        cptrsttoeod = np.nan\n",
    "        mwtopk = np.nan\n",
    "        mxhr = np.nan\n",
    "        idxmxhr = np.nan\n",
    "        nnantopk = np.nan\n",
    "        nnantoeod = np.nan\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        #find peak hourly rain on the day of the landslide \n",
    "\n",
    "        mxhr = ts.loc[lsdate:lsdate+datetime.timedelta(hours = 24)].max()\n",
    "       \n",
    "\n",
    "        #if peak hourly rain is nan (e.g. there's no data on the day of the landslide, then)\n",
    "\n",
    "        if pd.isnull(mxhr):\n",
    "            \n",
    "            \n",
    "            idxmxhr = np.nan\n",
    "            mxhr = np.nan\n",
    "            idxtrst = np.nan\n",
    "            htrsttopk = np.nan\n",
    "            cptrsttopk = np.nan\n",
    "            htrsttoeod = np.nan\n",
    "            cptrsttoeod = np.nan\n",
    "            mwtopk = np.nan\n",
    "            nnantopk = np.nan\n",
    "            nnantoeod = np.nan\n",
    "            \n",
    "\n",
    "        else:\n",
    "            \n",
    "            #find out when the peak occurred\n",
    "            idxmxhr = ts.loc[lsdate:lsdate+datetime.timedelta(hours = 24)].idxmax()\n",
    "\n",
    "\n",
    "            #moving window summing, index is last observation in window (eg. 10 am is 8am + 9am + 10 am)\n",
    "            mw = ts.rolling(nhdry).sum() \n",
    "\n",
    "            #subset moving window time series up to the peak \n",
    "            mwtopk = mw.loc[:idxmxhr]\n",
    "\n",
    "            # find where the nhdry hourly sum is less than 0.01 mm (essentially 0, but catch very small values)\n",
    "            #, find the nearest index to the peak point (iloc type index)\n",
    "            ix = mwtopk[mwtopk<0.01].index.get_indexer([idxmxhr], method = 'pad')[0]\n",
    "\n",
    "            #if we can't find a dry enough period in the time series:\n",
    "            if ix == -1:\n",
    "\n",
    "                idxtrst = np.nan\n",
    "                htrsttopk = np.nan\n",
    "                cptrsttopk = np.nan\n",
    "                htrsttoeod = np.nan\n",
    "                cptrsttoeod = np.nan\n",
    "                mwtopk = np.nan\n",
    "                nnantopk = np.nan\n",
    "                nnantoeod = np.nan\n",
    "                \n",
    "\n",
    "            else:\n",
    "\n",
    "                #get the time stamp index of the start of the triggering rainfall \n",
    "                idxtrst = mwtopk[mwtopk<0.01].index[ix] + datetime.timedelta(hours = 1)\n",
    "\n",
    "                #how long did it rain between the start of the triggering rain and the peak (inclusive)? \n",
    "\n",
    "                htrsttopk = ts.loc[idxtrst:idxmxhr].count()\n",
    "\n",
    "                #how much did it rain between the start of the triggering rain and the peak (inclusive)?\n",
    "                cptrsttopk = ts.loc[idxtrst:idxmxhr].sum()\n",
    "\n",
    "                #how long did it rain between the start of the triggering rain and the end of the landslide day? \n",
    "\n",
    "                htrsttoeod = ts.loc[idxtrst:lsdate+datetime.timedelta(hours = 24)].count()\n",
    "\n",
    "                #how much did it rain between the start of the triggering rain and the end of the landslide day?\n",
    "                cptrsttoeod = ts.loc[idxtrst:lsdate+datetime.timedelta(hours = 24)].sum()\n",
    "                \n",
    "                \n",
    "                #check what percent of the original time series leading up to the peak is nans\n",
    "                \n",
    "                nnantopk = ts_raw.loc[idxtrst:idxmxhr].isna().sum()\n",
    "                \n",
    "                #check what percent of the original time series leading up to the end of day is nans\n",
    "                \n",
    "                nnantoeod = ts_raw.loc[idxtrst:lsdate+datetime.timedelta(hours = 24)].isna().sum()\n",
    "                             \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "    return [idxmxhr, \n",
    "            mxhr, \n",
    "            idxtrst, \n",
    "            htrsttopk, \n",
    "            cptrsttopk, \n",
    "            htrsttoeod, \n",
    "            cptrsttoeod, \n",
    "            mwtopk, \n",
    "            nnantopk,\n",
    "            nnantoeod]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e87cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antecedent(ts, idxdt, h):\n",
    "    \n",
    "    \"\"\"\n",
    "    get cumulative antecedent precipitation before some hour (e.g. peak precip on landslide day \n",
    "    or before start of triggering rainfall)\n",
    "    \n",
    "    ts = time series\n",
    "    idxdt = datetime that you want antecedent for\n",
    "    h = number of antecedent hours\n",
    "    \n",
    "    \"\"\"\n",
    "    #for the case that there no start to the triggering rainfall was found\n",
    "    if pd.isnull(idxdt):\n",
    "        ante = np.nan\n",
    "        \n",
    "    #for the case that the time series is empty\n",
    "    elif len(ts) == 0:\n",
    "        ante = np.nan\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ante = ts.loc[idxdt - datetime.timedelta(hours = h):idxdt].sum()\n",
    "    \n",
    "    return ante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rainmetrics_durban(lspt, durban_rain):\n",
    "    \n",
    "    \"\"\"\n",
    "    get info about triggering, event, and antecedent rainfall for a landslide\n",
    "    \n",
    "    lspt - a dataframe row from lsdata\n",
    "    \n",
    "    \"\"\"\n",
    "    print(lspt.name)\n",
    "    \n",
    "    [ts, ts_fill] = prep_ts_durban(lspt, durban_rain, daysbefore = 90, daysafter = 1)\n",
    "    \n",
    "    if ts is None: \n",
    "        \n",
    "        rd = None\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        lsdate = lspt['date_local_midnight_utc']\n",
    "\n",
    "        #then, get triggering rainfall metrics\n",
    "\n",
    "        #define triggering event as 3 hours dry to peak or end of day\n",
    "        [idxmxhr, mxhr, idxtrst, \n",
    "         htrsttopk, cptrsttopk, htrsttoeod, cptrsttoeod, _, nnantrsttopk, nnantrsttoeod] = triggering_rain(nhdry = 3,\n",
    "                                                                                                   ts = ts_fill,\n",
    "                                                                                                    ts_raw = ts,\n",
    "                                                                                                   lsdate = lsdate)\n",
    "\n",
    "        #get event metrics \n",
    "\n",
    "        #define event as 48 hours dry to peak or end of day\n",
    "        [_, _, idxest, \n",
    "         hesttopk, cpesttopk, hesttoeod, cpesttoeod, _, nnanesttopk, nnanesttoeod] = triggering_rain(nhdry = 48,\n",
    "                                                                                ts = ts_fill,\n",
    "                                                                                ts_raw = ts,\n",
    "                                                                                lsdate = lsdate)\n",
    "\n",
    "\n",
    "        #get antecedent metrics \n",
    "\n",
    "        #24 hours before start of triggering rain \n",
    "\n",
    "        ante24htrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24)\n",
    "\n",
    "        #7 day before start of triggering rain \n",
    "        ante7dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*7)\n",
    "\n",
    "        #14 day before start of triggering rain \n",
    "        ante14dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*14)\n",
    "\n",
    "        #21 day before start of triggering rain \n",
    "        ante21dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*21)\n",
    "\n",
    "        #28 day before start of triggering rain\n",
    "        ante28dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*28)\n",
    "\n",
    "\n",
    "        #get estimated exceedance probabilities of the hourly maximum intensity\n",
    "\n",
    "       # mxhrexprob = get_exceedanceprob(ts, mxhr, 10) #minimum 10 years on record\n",
    "\n",
    "\n",
    "        #put everything into a dictionary to return \n",
    "\n",
    "\n",
    "        rd = {\"event_id\":lspt.name, \n",
    "             \"tr_start\":idxtrst,\n",
    "             \"tr_tpk\":idxmxhr,\n",
    "             \"tr_ppk\":mxhr,\n",
    "             \"tr_htopk\":htrsttopk, \n",
    "             \"tr_cptopk\":cptrsttopk, \n",
    "             \"tr_htoeod\":htrsttoeod, \n",
    "             \"tr_cptoeod\":cptrsttoeod,\n",
    "             \"tr_nnantopk\": nnantrsttopk, \n",
    "             \"tr_nnan_toeod\": nnantrsttoeod,\n",
    "             \"e_start\":idxest, \n",
    "             \"e_htopk\":hesttopk, \n",
    "             \"e_cptopk\":cpesttopk, \n",
    "             \"e_htoeod\":hesttoeod, \n",
    "             \"e_cptoeod\":cpesttoeod,\n",
    "             \"e_nnantopk\":nnanesttopk,\n",
    "             \"e_nnantoeod\":nnanesttoeod,               \n",
    "             \"tr_ante24h\":ante24htrst, \n",
    "             \"tr_ante7d\":ante7dtrst, \n",
    "             \"tr_ante14d\":ante14dtrst, \n",
    "             \"tr_ante21d\":ante21dtrst, \n",
    "             \"tr_ante28d\":ante28dtrst}\n",
    "\n",
    "    print(lspt.name)\n",
    "\n",
    "    return rd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c36732",
   "metadata": {},
   "source": [
    "Get the gauges within a 25 km radius of the landslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(durban_landslides)):\n",
    "       \n",
    "    lspt = durban_landslides.iloc[l]\n",
    "    \n",
    "    tempdf = get_durban_stations(lspt, \n",
    "                durban_stations,\n",
    "                buffer_dist = 25000)\n",
    "    \n",
    "    if l == 0:\n",
    "        \n",
    "        gageinfo = tempdf.copy()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        gageinfo = pd.concat([gageinfo, tempdf], axis = 0)\n",
    "    \n",
    "    print('end {}/{}'.format(l, len(durban_landslides)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join to landslides dataframe (right join)\n",
    "\n",
    "\n",
    "durban_ls_gages = durban_landslides.merge(gageinfo, \n",
    "                           how = 'right',\n",
    "                            left_index = True,\n",
    "                           right_on = 'lsidx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7d8aa",
   "metadata": {},
   "source": [
    "get rain metrics for each landslide/gauge combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record = True):\n",
    "    durban_ls_gages['rain_metrics'] = durban_ls_gages.apply(lambda lspt:get_rainmetrics_durban(lspt, durban_rain), \n",
    "                                             axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e5c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "joindf = pd.DataFrame(list(durban_ls_gages['rain_metrics'].values))\n",
    "\n",
    "lsdata_durban_rain = pd.concat([durban_ls_gages, \n",
    "             joindf.set_index(durban_ls_gages.index)], \n",
    "             axis = 1)\n",
    "\n",
    "lsdata_durban_rain.drop(['rain_metrics'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "#strip city names of white space, special characters, etc for R\n",
    "\n",
    "lsdata_durban_rain['city'] = lsdata_durban_rain.apply(lambda row:''.join(e for e in row['UC_NM_MN'] if e.isalnum()), \n",
    "                                       axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename some columns to align with gsdr format to be able to concatenate later\n",
    "\n",
    "lsdata_durban_rain.rename(columns = {'latitude':'Latitude', \n",
    "                                     'longitude':'Longitude',\n",
    "                                     'station_name':'OriginalID'}, \n",
    "                           inplace = True)\n",
    "\n",
    "\n",
    "#add some fields of nans to be able to concatenate to gsdr later\n",
    "\n",
    "lsdata_durban_rain['Folder'] = np.nan \n",
    "lsdata_durban_rain['NewID'] = np.nan\n",
    "lsdata_durban_rain['Recordlength(hours)'] = np.nan\n",
    "lsdata_durban_rain['Recordlength(years)'] = np.nan\n",
    "lsdata_durban_rain['StartDate'] = np.nan\n",
    "lsdata_durban_rain['EndDate'] = np.nan\n",
    "lsdata_durban_rain['Missingdata(%)'] = np.nan\n",
    "lsdata_durban_rain['coverage'] = np.nan\n",
    "lsdata_durban_rain['precip_source'] = 'SAWS'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10031738",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdata_durban_rain_save = lsdata_durban_rain.loc[:,['inventory', 'src_index', 'inventory_id', 'inventory_id_name', 'lsidx',\n",
    "       'ID_HDC_G0', 'UC_NM_MN', 'date_local_midnight_utc', 'Folder',\n",
    "       'OriginalID', 'NewID', 'Latitude', 'Longitude', 'Recordlength(hours)',\n",
    "       'Recordlength(years)', 'StartDate', 'EndDate', 'Missingdata(%)',\n",
    "       'geometry_y', 'flag', 'coverage', 'station_dist', 'event_id',\n",
    "       'tr_start', 'tr_tpk', 'tr_ppk', 'tr_htopk', 'tr_cptopk', 'tr_htoeod',\n",
    "       'tr_cptoeod', 'tr_nnantopk', 'tr_nnan_toeod', 'e_start', 'e_htopk',\n",
    "       'e_cptopk', 'e_htoeod', 'e_cptoeod', 'e_nnantopk', 'e_nnantoeod',\n",
    "       'tr_ante24h', 'tr_ante7d', 'tr_ante14d', 'tr_ante21d', 'tr_ante28d',\n",
    "       'city', 'precip_source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa10b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdata_durban_rain_save.to_csv(resultsdir + 'lsdata_durban_rain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc647766",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdata_durban_rain_save = pd.read_csv(resultsdir + 'lsdata_durban_rain.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c7bb9",
   "metadata": {},
   "source": [
    "### get annual maxima for Durban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_whole_ts_durban(station):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to read the precipitation time series from a gauge, convert it to UTC,\n",
    "    interpolate missing values\n",
    "    \n",
    "    station = row of a dataframe that contains columns: \n",
    "        - 'Folder' - folder containing the data (e.g. US.zip)\n",
    "        - 'OriginalID' - name of station\n",
    "    gsdrdir = directory containing the GSDR subfolders\n",
    "    \n",
    "    \n",
    "    Returns: \n",
    "    ts = original time series\n",
    "    ts_fill = original time series, interpolated   \n",
    "    \n",
    "    \"\"\"\n",
    "            \n",
    "    ts = durban_rain[durban_rain['Station_Name'] == station]['Rain']\n",
    "    \n",
    "\n",
    "    #already in UTC\n",
    "\n",
    "    #sort index, just in case for some reason it wasn't already in sorted, or pandas thinks it isn't\n",
    "    ts.sort_index(axis = 0, inplace = True)\n",
    "    \n",
    "    #interpolate to fill nans in the time series using the pandas built in interpolation method\n",
    "\n",
    "    ts_fill = ts.interpolate(method = 'time')\n",
    "\n",
    "    #remove any duplicates by keeping the first duplicated entry only\n",
    "    \n",
    "    ts_fill = ts_fill[~ts_fill.index.duplicated()]\n",
    "\n",
    "    return ts, ts_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [1, 3, 6, 12, 24, 48, 100, 200, 500, 1000] #durations to extract block maxima for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record = True):\n",
    "#loop through all stations associated with a landslide and extract annual block maxima at a range of durations\n",
    "\n",
    "    #loop over stations\n",
    "\n",
    "    for s in range(len(uniquestations)):\n",
    "        \n",
    "        print(s)\n",
    "        \n",
    "        station = lsdata_durban_rain_save[lsdata_durban_rain_save['OriginalID'] == uniquestations[s]].iloc[0][['ID_HDC_G0', \n",
    "                                                                                          'UC_NM_MN', \n",
    "                                                                                          'Folder', \n",
    "                                                                                          'OriginalID', \n",
    "                                                                                           'NewID', \n",
    "                                                                                          'Latitude',\n",
    "                                                                                          'Longitude',\n",
    "                                                                                          'Recordlength(hours)',\n",
    "                                                                                          'Recordlength(years)',\n",
    "                                                                                          'StartDate',\n",
    "                                                                                          'EndDate',\n",
    "                                                                                          'Missingdata(%)',\n",
    "                                                                                          'geometry_y']]\n",
    "        \n",
    "\n",
    "        #get time series\n",
    "\n",
    "        ts, ts_fill = prep_whole_ts_durban(uniquestations[s])\n",
    "\n",
    "        #extract block maxima at a range of durations\n",
    "\n",
    "        for d in range(len(durations)):\n",
    "\n",
    "           #moving window on the raw time series\n",
    "            mw = ts.rolling(durations[d]).sum() #right index is sum of previous d observations. \n",
    "            #if there are nans within the window, returns nan\n",
    "\n",
    "            ann_block_max = mw.resample('Y').max() #take annual max. ignores nans (could be missing data, \n",
    "            #will still get max)\n",
    "\n",
    "\n",
    "            #moving window on the interpolated time series\n",
    "\n",
    "            mw_fill = ts_fill.rolling(durations[d]).sum() #right index is sum of previous d observations. \n",
    "            #if there are nans within the window, returns nan\n",
    "\n",
    "            ann_block_max_fill = mw_fill.resample('Y').max() #take annual max. ignores nans (could be missing data, \n",
    "            #will still get max)\n",
    "            \n",
    "            block_max_df = pd.DataFrame(ann_block_max.values, index = ann_block_max.index, columns = ['raw_block_max'])\n",
    "            #block_max_df = pd.DataFrame(ann_block_max, columns = ['raw_block_max'])\n",
    "            block_max_df['fill_block_max'] = ann_block_max_fill.values\n",
    "\n",
    "            #record the number of non-nan observations in the moving window (all other hours in the year are then nan)\n",
    "            block_max_df['raw_notnainmw'] = mw.resample('Y').count().values\n",
    "\n",
    "            #record the number of non-nan observations in the raw time series (all other hours in the year are then nan)\n",
    "            block_max_df['raw_notnaints'] = ts.resample('Y').count().values\n",
    "\n",
    "\n",
    "            block_max_df['duration(h)'] = durations[d]\n",
    "            block_max_df['year'] = block_max_df.index.year\n",
    "\n",
    "\n",
    "            if d == 0:\n",
    "\n",
    "                stationdf = block_max_df.copy() \n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                stationdf = pd.concat([stationdf, block_max_df])\n",
    "\n",
    "\n",
    "        #assign information about the station\n",
    "        stationinfo = pd.DataFrame([station])\n",
    "\n",
    "        stationinfo = stationinfo.loc[stationinfo.index.repeat(len(stationdf))]\n",
    "\n",
    "        stationinfo.set_index(stationdf.index, inplace = True)\n",
    "\n",
    "        stationdf = pd.concat([stationinfo, stationdf], axis = 1)\n",
    "\n",
    "        if s == 0: \n",
    "\n",
    "            maindf = stationdf.copy()\n",
    "\n",
    "        else: \n",
    "\n",
    "            maindf = pd.concat([maindf, stationdf])\n",
    "            \n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87befff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "maindf.to_csv(resultsdir + 'annual_block_maxima_durban.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8d10b",
   "metadata": {},
   "source": [
    "### Read and process rainfall data from Medellin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "medellin_stations = pd.read_csv(\"Stations_IDEAM_Hourly.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "medellin_stations = gpd.GeoDataFrame(medellin_stations,\n",
    "                                 geometry=gpd.points_from_xy(medellin_stations.longitud, medellin_stations.latitud),\n",
    "                                 crs = \"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf90163",
   "metadata": {},
   "source": [
    "Merge IDEAM data into one pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideam_dir = '../Rainfall_IDEAM/Hourly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(medellin_stations)):\n",
    "    \n",
    "    fn = ideam_dir + str(medellin_stations.iloc[i]['CODIGO']) + '.csv'\n",
    "    print(fn)\n",
    "    \n",
    "    tempdf = pd.read_csv(fn)\n",
    "    \n",
    "    if i == 0: \n",
    "        medellin_rain = tempdf.copy()\n",
    "        \n",
    "    else:\n",
    "        medellin_rain = pd.concat([medellin_rain, \n",
    "                               tempdf], \n",
    "                               axis = 0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89855db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "medellin_rain = medellin_rain.set_index(pd.to_datetime(medellin_rain['Fecha'], dayfirst = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171db6f0",
   "metadata": {},
   "source": [
    "Convert local time stamps to UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb482f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate timezone finder\n",
    "tf = TimezoneFinder()\n",
    "\n",
    "stz = tf.timezone_at(lng=medellin_rain.iloc[0].Longitud, lat=medellin_rain.iloc[0].Latitud)\n",
    "\n",
    "#first, localize to local time zone\n",
    "medellin_rain = medellin_rain.tz_localize(stz, \n",
    "                        ambiguous=np.zeros(len(medellin_rain), dtype = bool), \n",
    "                        nonexistent ='shift_forward')\n",
    "#then, convert to UTC\n",
    "\n",
    "medellin_rain = medellin_rain.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280b3fc",
   "metadata": {},
   "source": [
    "get Medellin landslides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "medellin_landslides = landslides[landslides['UC_NM_MN'] == 'Medellín'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801e262",
   "metadata": {},
   "source": [
    "Functions for Medellin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function \n",
    "\n",
    "def get_medellin_stations(lspt, \n",
    "                medellin_stations,\n",
    "                buffer_dist = 25000):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to get the all of the IDEAM stations \n",
    "    within a defined buffer distance to a landslide point\n",
    "    \n",
    "    lspt = a landslide point (with a DATE and spatial location in WGS84)\n",
    "    medellin_stations = pandas dataframe with a geometry in WGS84\n",
    "    buffer_dist = buffer distance in meters\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "\n",
    "    #define azimuthal equidistant crs centered on the landslide\n",
    "\n",
    "    aeqd = pyproj.Proj(proj='aeqd', ellps='WGS84', datum='WGS84', lat_0=lspt.geometry.y, lon_0=lspt.geometry.x).srs\n",
    "\n",
    "    #reproject the landslide point\n",
    "\n",
    "    project = pyproj.Transformer.from_crs(wgs84, aeqd, always_xy=True).transform\n",
    "    lspt_a = transform(project, lspt.geometry)\n",
    "\n",
    "\n",
    "    #buffer the landlide point\n",
    "    lspt_ab = lspt_a.buffer(buffer_dist)\n",
    "\n",
    "    #reproject the station data \n",
    "    ga = medellin_stations.to_crs(aeqd)\n",
    "\n",
    "    #get the indices of all stations within 25 km of the landslide point\n",
    "    sidx = ga.sindex.query(lspt_ab, predicate = 'intersects')\n",
    "    \n",
    "    #columns of the metadata dataframe to use as dictionary keys\n",
    "    \n",
    "    gscols = medellin_stations.columns\n",
    "    \n",
    "    #check if it's empty\n",
    "    \n",
    "    if len(sidx) == 0: \n",
    "        \n",
    "        gageinfo = dict(zip(gscols, [None]*len(gscols)))\n",
    "        \n",
    "        gageinfo['flag'] = 'no close gages'\n",
    "        \n",
    "        gageinfodf = pd.DataFrame([gageinfo])\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        #continue\n",
    "\n",
    "        #these are the stations within the buffer distance of the landslide point\n",
    "        gs = ga.iloc[sidx].copy()\n",
    "\n",
    "        #get the distance from the stations to the landslide point\n",
    "        dsl = gs.distance(lspt_a)\n",
    "\n",
    "        #record the distance\n",
    "        gs.loc[dsl.index, 'station_dist'] = dsl.values \n",
    "\n",
    "        #project stations back to WGS84\n",
    "\n",
    "        gs = gs.to_crs(wgs84)\n",
    "\n",
    "        #return the info for the nearest station \n",
    "\n",
    "        gageinfo = gs\n",
    "\n",
    "        gageinfo['flag'] = 'coverage'\n",
    "\n",
    "        gageinfodf = gageinfo.copy()\n",
    "            \n",
    "    gageinfodf['lsidx'] = lspt.name\n",
    "                \n",
    "    return gageinfodf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ts_medellin(lspt, medellin_rain, daysbefore = 90, daysafter = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to interpolate missing values, and subset to the specified time interval before\n",
    "    and after the landslide point\n",
    "    \n",
    "    lspt = row of a dataframe that contains columns: \n",
    "        - CODIGO - station name\n",
    "        - Valor - precipitation value\n",
    "        - 'date_local_midnight_utc' - midnight local time on the day the landslide occurred, converted to UTC\n",
    "    \n",
    "    medellin_rain = pandas dataframe of medellin rain\n",
    "    daysbefore = how many days before the landslide should data be extracted?\n",
    "    daysafter = how many days after midnight on the day of the landslide should be extracted? \n",
    "    \n",
    "    Returns: \n",
    "    ts_sub = original time series, subset\n",
    "    ts_fill = original time series, subset and interpolated   \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #get the time series from the correct station\n",
    "    ts = medellin_rain[medellin_rain['CodigoEstacion'] == lspt['CODIGO']]['Valor'].copy()\n",
    "    \n",
    "    \n",
    "    #sort index\n",
    "    ts = ts.sort_index()\n",
    "\n",
    "    #subset to specified number of days before and after the landslide\n",
    "    lsdate = lspt['date_local_midnight_utc']\n",
    "\n",
    "    #check that these dates are in the range of the time series\n",
    "\n",
    "    start_ts = lsdate - datetime.timedelta(days = daysbefore)\n",
    "    end_ts = lsdate + datetime.timedelta(days = daysafter + 1)\n",
    "    \n",
    "    if (start_ts > ts.index[0]) & (end_ts < ts.index[-1]):\n",
    "\n",
    "        ts_sub =  ts.loc[(lsdate - datetime.timedelta(days = daysbefore)):(lsdate + datetime.timedelta(days = daysafter + 1))]\n",
    "\n",
    "        #sort index, just in case for some reason it wasn't already in sorted, or pandas thinks it isn't\n",
    "        ts_sub.sort_index(axis = 0, inplace = True)\n",
    "\n",
    "        #interpolate to fill nans in the time series using the pandas built in interpolation method\n",
    "\n",
    "\n",
    "        ts_fill = ts_sub.interpolate(method = 'time')\n",
    "\n",
    "        #remove any duplicates by keeping the first duplicated entry only\n",
    "        ts_fill = ts_fill[~ts_fill.index.duplicated()]\n",
    "    \n",
    "    else: #case that the time series can't be extracted because time series is not completely covered\n",
    "        ts_sub = np.array([])\n",
    "        ts_fill = np.array([])\n",
    "        \n",
    "\n",
    "    return ts_sub, ts_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triggering_rain(nhdry, ts, ts_raw, lsdate): \n",
    "    \n",
    "    \"\"\"\n",
    "    Gets length and sum of triggering rainfall event from a dry period of length nhdry to the peak hourly\n",
    "    rainfall and the end of the landslide day\n",
    "    \n",
    "    nhdry = length of dry period preceeding landslide in hours\n",
    "    ts = time series with hourly datetime index and values of hourly precip\n",
    "    ts_raw = original time series to check how many nans were interpolated in the time range\n",
    "    lsdate = datetime corresponding to the start of the landslide day  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #if the time series is completely empty\n",
    "    \n",
    "    if len(ts) == 0:\n",
    "        \n",
    "        idxtrst = np.nan\n",
    "        htrsttopk = np.nan\n",
    "        cptrsttopk = np.nan\n",
    "        htrsttoeod = np.nan\n",
    "        cptrsttoeod = np.nan\n",
    "        mwtopk = np.nan\n",
    "        mxhr = np.nan\n",
    "        idxmxhr = np.nan\n",
    "        nnantopk = np.nan\n",
    "        nnantoeod = np.nan\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        #find peak hourly rain on the day of the landslide \n",
    "\n",
    "        mxhr = ts.loc[lsdate:lsdate+datetime.timedelta(hours = 24)].max()\n",
    "       \n",
    "\n",
    "        #if peak hourly rain is nan (e.g. there's no data on the day of the landslide, then)\n",
    "\n",
    "        if pd.isnull(mxhr):\n",
    "            \n",
    "            \n",
    "            idxmxhr = np.nan\n",
    "            mxhr = np.nan\n",
    "            idxtrst = np.nan\n",
    "            htrsttopk = np.nan\n",
    "            cptrsttopk = np.nan\n",
    "            htrsttoeod = np.nan\n",
    "            cptrsttoeod = np.nan\n",
    "            mwtopk = np.nan\n",
    "            nnantopk = np.nan\n",
    "            nnantoeod = np.nan\n",
    "            \n",
    "\n",
    "        else:\n",
    "            \n",
    "            #find out when the peak occurred\n",
    "            idxmxhr = ts.loc[lsdate:lsdate+datetime.timedelta(hours = 24)].idxmax()\n",
    "\n",
    "\n",
    "            #moving window summing, index is last observation in window (eg. 10 am is 8am + 9am + 10 am)\n",
    "            mw = ts.rolling(nhdry).sum() \n",
    "\n",
    "            #subset moving window time series up to the peak \n",
    "            mwtopk = mw.loc[:idxmxhr]\n",
    "\n",
    "            # find where the nhdry hourly sum is less than 0.01 mm (essentially 0, but catch very small values)\n",
    "            #, find the nearest index to the peak point (iloc type index)\n",
    "            ix = mwtopk[mwtopk<0.01].index.get_indexer([idxmxhr], method = 'pad')[0]\n",
    "\n",
    "            #if we can't find a dry enough period in the time series:\n",
    "            if ix == -1:\n",
    "\n",
    "                idxtrst = np.nan\n",
    "                htrsttopk = np.nan\n",
    "                cptrsttopk = np.nan\n",
    "                htrsttoeod = np.nan\n",
    "                cptrsttoeod = np.nan\n",
    "                mwtopk = np.nan\n",
    "                nnantopk = np.nan\n",
    "                nnantoeod = np.nan\n",
    "                \n",
    "\n",
    "            else:\n",
    "\n",
    "                #get the time stamp index of the start of the triggering rainfall \n",
    "                idxtrst = mwtopk[mwtopk<0.01].index[ix] + datetime.timedelta(hours = 1)\n",
    "\n",
    "                #how long did it rain between the start of the triggering rain and the peak (inclusive)? \n",
    "\n",
    "                htrsttopk = ts.loc[idxtrst:idxmxhr].count()\n",
    "\n",
    "                #how much did it rain between the start of the triggering rain and the peak (inclusive)?\n",
    "                cptrsttopk = ts.loc[idxtrst:idxmxhr].sum()\n",
    "\n",
    "                #how long did it rain between the start of the triggering rain and the end of the landslide day? \n",
    "\n",
    "                htrsttoeod = ts.loc[idxtrst:lsdate+datetime.timedelta(hours = 24)].count()\n",
    "\n",
    "                #how much did it rain between the start of the triggering rain and the end of the landslide day?\n",
    "                cptrsttoeod = ts.loc[idxtrst:lsdate+datetime.timedelta(hours = 24)].sum()\n",
    "                \n",
    "                \n",
    "                #check what percent of the original time series leading up to the peak is nans\n",
    "                \n",
    "                nnantopk = ts_raw.loc[idxtrst:idxmxhr].isna().sum()\n",
    "                \n",
    "                #check what percent of the original time series leading up to the end of day is nans\n",
    "                \n",
    "                nnantoeod = ts_raw.loc[idxtrst:lsdate+datetime.timedelta(hours = 24)].isna().sum()\n",
    "                             \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "    return [idxmxhr, \n",
    "            mxhr, \n",
    "            idxtrst, \n",
    "            htrsttopk, \n",
    "            cptrsttopk, \n",
    "            htrsttoeod, \n",
    "            cptrsttoeod, \n",
    "            mwtopk, \n",
    "            nnantopk,\n",
    "            nnantoeod]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antecedent(ts, idxdt, h):\n",
    "    \n",
    "    \"\"\"\n",
    "    get cumulative antecedent precipitation before some hour (e.g. peak precip on landslide day \n",
    "    or before start of triggering rainfall)\n",
    "    \n",
    "    ts = time series\n",
    "    idxdt = datetime that you want antecedent for\n",
    "    h = number of antecedent hours\n",
    "    \n",
    "    \"\"\"\n",
    "    #for the case that there no start to the triggering rainfall was found\n",
    "    if pd.isnull(idxdt):\n",
    "        ante = np.nan\n",
    "        \n",
    "    #for the case that the time series is empty\n",
    "    elif len(ts) == 0:\n",
    "        ante = np.nan\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ante = ts.loc[idxdt - datetime.timedelta(hours = h):idxdt].sum()\n",
    "    \n",
    "    return ante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14477540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rainmetrics_medellin(lspt, medellin_rain):\n",
    "    \n",
    "    \"\"\"\n",
    "    get info about triggering, event, and antecedent rainfall for a landslide\n",
    "    \n",
    "    lspt - a dataframe row from lsdata\n",
    "    \n",
    "    \"\"\"\n",
    "    print(lspt.name)\n",
    "    \n",
    "    [ts, ts_fill] = prep_ts_medellin(lspt, medellin_rain, daysbefore = 90, daysafter = 1)\n",
    "    \n",
    "    if ts is None: \n",
    "        \n",
    "        rd = None\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        lsdate = lspt['date_local_midnight_utc']\n",
    "\n",
    "        #then, get triggering rainfall metrics\n",
    "\n",
    "        #define triggering event as 3 hours dry to peak or end of day\n",
    "        [idxmxhr, mxhr, idxtrst, \n",
    "         htrsttopk, cptrsttopk, htrsttoeod, cptrsttoeod, _, nnantrsttopk, nnantrsttoeod] = triggering_rain(nhdry = 3,\n",
    "                                                                                                   ts = ts_fill,\n",
    "                                                                                                    ts_raw = ts,\n",
    "                                                                                                   lsdate = lsdate)\n",
    "\n",
    "        #get event metrics \n",
    "\n",
    "        #define event as 48 hours dry to peak or end of day\n",
    "        [_, _, idxest, \n",
    "         hesttopk, cpesttopk, hesttoeod, cpesttoeod, _, nnanesttopk, nnanesttoeod] = triggering_rain(nhdry = 48,\n",
    "                                                                                ts = ts_fill,\n",
    "                                                                                ts_raw = ts,\n",
    "                                                                                lsdate = lsdate)\n",
    "\n",
    "\n",
    "        #get antecedent metrics \n",
    "\n",
    "        #24 hours before start of triggering rain \n",
    "\n",
    "        ante24htrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24)\n",
    "\n",
    "        #7 day before start of triggering rain \n",
    "        ante7dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*7)\n",
    "\n",
    "        #14 day before start of triggering rain \n",
    "        ante14dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*14)\n",
    "\n",
    "        #21 day before start of triggering rain \n",
    "        ante21dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*21)\n",
    "\n",
    "        #28 day before start of triggering rain\n",
    "        ante28dtrst = get_antecedent(ts = ts_fill, idxdt = idxtrst, h = 24*28)\n",
    "\n",
    "\n",
    "        #get estimated exceedance probabilities of the hourly maximum intensity\n",
    "\n",
    "       # mxhrexprob = get_exceedanceprob(ts, mxhr, 10) #minimum 10 years on record\n",
    "\n",
    "\n",
    "        #put everything into a dictionary to return \n",
    "\n",
    "\n",
    "        rd = {\"event_id\":lspt.name, \n",
    "             \"tr_start\":idxtrst,\n",
    "             \"tr_tpk\":idxmxhr,\n",
    "             \"tr_ppk\":mxhr,\n",
    "             \"tr_htopk\":htrsttopk, \n",
    "             \"tr_cptopk\":cptrsttopk, \n",
    "             \"tr_htoeod\":htrsttoeod, \n",
    "             \"tr_cptoeod\":cptrsttoeod,\n",
    "             \"tr_nnantopk\": nnantrsttopk, \n",
    "             \"tr_nnan_toeod\": nnantrsttoeod,\n",
    "             \"e_start\":idxest, \n",
    "             \"e_htopk\":hesttopk, \n",
    "             \"e_cptopk\":cpesttopk, \n",
    "             \"e_htoeod\":hesttoeod, \n",
    "             \"e_cptoeod\":cpesttoeod,\n",
    "             \"e_nnantopk\":nnanesttopk,\n",
    "             \"e_nnantoeod\":nnanesttoeod,               \n",
    "             \"tr_ante24h\":ante24htrst, \n",
    "             \"tr_ante7d\":ante7dtrst, \n",
    "             \"tr_ante14d\":ante14dtrst, \n",
    "             \"tr_ante21d\":ante21dtrst, \n",
    "             \"tr_ante28d\":ante28dtrst}\n",
    "\n",
    "    print(lspt.name)\n",
    "\n",
    "    return rd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8e288",
   "metadata": {},
   "source": [
    "Get the gauges within a 25 km radius of the landslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890afb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(medellin_landslides)):\n",
    "       \n",
    "    lspt = medellin_landslides.iloc[l]\n",
    "    \n",
    "    tempdf = get_medellin_stations(lspt, \n",
    "                medellin_stations,\n",
    "                buffer_dist = 25000)\n",
    "    \n",
    "    if l == 0:\n",
    "        \n",
    "        gageinfo = tempdf.copy()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        gageinfo = pd.concat([gageinfo, tempdf], axis = 0)\n",
    "    \n",
    "    print('end {}/{}'.format(l, len(medellin_landslides)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join to landslides dataframe (right join)\n",
    "\n",
    "\n",
    "medellin_ls_gages = medellin_landslides.merge(gageinfo, \n",
    "                           how = 'right',\n",
    "                            left_index = True,\n",
    "                           right_on = 'lsidx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc5f5f",
   "metadata": {},
   "source": [
    "get rain metrics for each landslide/gauge combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "medellin_ls_gages['rain_metrics'] = medellin_ls_gages.apply(lambda lspt:get_rainmetrics_medellin(lspt, medellin_rain), \n",
    "                                             axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9358229",
   "metadata": {},
   "outputs": [],
   "source": [
    "joindf = pd.DataFrame(list(medellin_ls_gages['rain_metrics'].values))\n",
    "\n",
    "lsdata_medellin_rain = pd.concat([medellin_ls_gages, \n",
    "             joindf.set_index(medellin_ls_gages.index)], \n",
    "             axis = 1)\n",
    "\n",
    "lsdata_medellin_rain.drop(['rain_metrics'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "#strip city names of white space, special characters, etc for R\n",
    "\n",
    "lsdata_medellin_rain['city'] = lsdata_medellin_rain.apply(lambda row:''.join(e for e in row['UC_NM_MN'] if e.isalnum()), \n",
    "                                       axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143285cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename some columns to align with gsdr format to be able to concatenate later\n",
    "\n",
    "lsdata_medellin_rain.rename(columns = {'latitud':'Latitude', \n",
    "                                      'longitud':'Longitude', \n",
    "                                      'CODIGO':'OriginalID'}, \n",
    "                           inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add some fields of nans to be able to concatenate to gsdr later\n",
    "\n",
    "lsdata_medellin_rain['Folder'] = np.nan \n",
    "lsdata_medellin_rain['NewID'] = np.nan\n",
    "lsdata_medellin_rain['Recordlength(hours)'] = np.nan\n",
    "lsdata_medellin_rain['Recordlength(years)'] = np.nan\n",
    "lsdata_medellin_rain['StartDate'] = np.nan\n",
    "lsdata_medellin_rain['EndDate'] = np.nan\n",
    "lsdata_medellin_rain['Missingdata(%)'] = np.nan\n",
    "lsdata_medellin_rain['coverage'] = np.nan\n",
    "lsdata_medellin_rain['precip_source'] = 'IDEAM'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdata_medellin_rain_save = lsdata_medellin_rain.loc[:,['inventory', 'src_index', 'inventory_id', 'inventory_id_name', 'lsidx',\n",
    "       'ID_HDC_G0', 'UC_NM_MN', 'date_local_midnight_utc', 'Folder',\n",
    "       'OriginalID', 'NewID', 'Latitude', 'Longitude', 'Recordlength(hours)',\n",
    "       'Recordlength(years)', 'StartDate', 'EndDate', 'Missingdata(%)',\n",
    "       'geometry_y', 'flag', 'coverage', 'station_dist', 'event_id',\n",
    "       'tr_start', 'tr_tpk', 'tr_ppk', 'tr_htopk', 'tr_cptopk', 'tr_htoeod',\n",
    "       'tr_cptoeod', 'tr_nnantopk', 'tr_nnan_toeod', 'e_start', 'e_htopk',\n",
    "       'e_cptopk', 'e_htoeod', 'e_cptoeod', 'e_nnantopk', 'e_nnantoeod',\n",
    "       'tr_ante24h', 'tr_ante7d', 'tr_ante14d', 'tr_ante21d', 'tr_ante28d',\n",
    "       'city', 'precip_source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7311fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdata_medellin_rain_save.to_csv(resultsdir + 'lsdata_medellin_rain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbd8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdata_medellin_rain_save = pd.read_csv(resultsdir + 'lsdata_medellin_rain.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0dc2f5",
   "metadata": {},
   "source": [
    "### Extract annual maxima for Medellin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "medellin_rain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_whole_ts_medellin(station):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to read the precipitation time series from a gauge, convert it to UTC,\n",
    "    interpolate missing values\n",
    "    \n",
    "    station = row of a dataframe that contains columns: \n",
    "        - 'Folder' - folder containing the data (e.g. US.zip)\n",
    "        - 'OriginalID' - name of station\n",
    "    gsdrdir = directory containing the GSDR subfolders\n",
    "    \n",
    "    \n",
    "    Returns: \n",
    "    ts = original time series\n",
    "    ts_fill = original time series, interpolated   \n",
    "    \n",
    "    \"\"\"\n",
    "            \n",
    "    ts = medellin_rain[medellin_rain['CodigoEstacion'] == station]['Valor']\n",
    "    \n",
    "\n",
    "    #already in UTC\n",
    "\n",
    "    #sort index, just in case for some reason it wasn't already in sorted, or pandas thinks it isn't\n",
    "    ts.sort_index(axis = 0, inplace = True)\n",
    "    \n",
    "    #interpolate to fill nans in the time series using the pandas built in interpolation method\n",
    "\n",
    "    ts_fill = ts.interpolate(method = 'time')\n",
    "\n",
    "    #remove any duplicates by keeping the first duplicated entry only\n",
    "   \n",
    "    ts_fill = ts_fill[~ts_fill.index.duplicated()]\n",
    "\n",
    "    return ts, ts_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647587ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquestations = lsdata_medellin_rain_save['OriginalID'].unique() #stations with a landslide associated with it\n",
    "\n",
    "durations = [1, 3, 6, 12, 24, 48, 100, 200, 500, 1000] #durations to extract block maxima for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02cc182",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record = True):\n",
    "#loop through all stations associated with a landslide and extract annual block maxima at a range of durations\n",
    "\n",
    "    #loop over stations\n",
    "\n",
    "    for s in range(len(uniquestations)):\n",
    "        \n",
    "        print(s)\n",
    "        \n",
    "        station = lsdata_medellin_rain_save[lsdata_medellin_rain_save['OriginalID'] == uniquestations[s]].iloc[0][['ID_HDC_G0', \n",
    "                                                                                          'UC_NM_MN', \n",
    "                                                                                          'Folder', \n",
    "                                                                                          'OriginalID', \n",
    "                                                                                           'NewID', \n",
    "                                                                                          'Latitude',\n",
    "                                                                                          'Longitude',\n",
    "                                                                                          'Recordlength(hours)',\n",
    "                                                                                          'Recordlength(years)',\n",
    "                                                                                          'StartDate',\n",
    "                                                                                          'EndDate',\n",
    "                                                                                          'Missingdata(%)',\n",
    "                                                                                          'geometry_y']]\n",
    "        \n",
    "\n",
    "        #get time series\n",
    "\n",
    "        ts, ts_fill = prep_whole_ts_medellin(uniquestations[s])\n",
    "\n",
    "        #extract block maxima at a range of durations\n",
    "\n",
    "        for d in range(len(durations)):\n",
    "\n",
    "           #moving window on the raw time series\n",
    "            mw = ts.rolling(durations[d]).sum() #right index is sum of previous d observations. \n",
    "            #if there are nans within the window, returns nan\n",
    "\n",
    "            ann_block_max = mw.resample('Y').max() #take annual max. ignores nans (could be missing data, \n",
    "            #will still get max)\n",
    "\n",
    "\n",
    "            #moving window on the interpolated time series\n",
    "\n",
    "            mw_fill = ts_fill.rolling(durations[d]).sum() #right index is sum of previous d observations. \n",
    "            #if there are nans within the window, returns nan\n",
    "\n",
    "            ann_block_max_fill = mw_fill.resample('Y').max() #take annual max. ignores nans (could be missing data, \n",
    "            #will still get max)\n",
    "            \n",
    "            block_max_df = pd.DataFrame(ann_block_max.values, index = ann_block_max.index, columns = ['raw_block_max'])\n",
    "            #block_max_df = pd.DataFrame(ann_block_max, columns = ['raw_block_max'])\n",
    "            block_max_df['fill_block_max'] = ann_block_max_fill.values\n",
    "\n",
    "            #record the number of non-nan observations in the moving window (all other hours in the year are then nan)\n",
    "            block_max_df['raw_notnainmw'] = mw.resample('Y').count().values\n",
    "\n",
    "            #record the number of non-nan observations in the raw time series (all other hours in the year are then nan)\n",
    "            block_max_df['raw_notnaints'] = ts.resample('Y').count().values\n",
    "\n",
    "\n",
    "            block_max_df['duration(h)'] = durations[d]\n",
    "            block_max_df['year'] = block_max_df.index.year\n",
    "\n",
    "\n",
    "            if d == 0:\n",
    "\n",
    "                stationdf = block_max_df.copy() \n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                stationdf = pd.concat([stationdf, block_max_df])\n",
    "\n",
    "\n",
    "        #assign information about the station\n",
    "        stationinfo = pd.DataFrame([station])\n",
    "\n",
    "        stationinfo = stationinfo.loc[stationinfo.index.repeat(len(stationdf))]\n",
    "\n",
    "        stationinfo.set_index(stationdf.index, inplace = True)\n",
    "\n",
    "        stationdf = pd.concat([stationinfo, stationdf], axis = 1)\n",
    "\n",
    "        if s == 0: \n",
    "\n",
    "            maindf = stationdf.copy()\n",
    "\n",
    "        else: \n",
    "\n",
    "            maindf = pd.concat([maindf, stationdf])\n",
    "            \n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04aba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "maindf.to_csv(resultsdir + 'annual_block_maxima_medellin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15c668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
